{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities import url_to_corpus\n",
    "\n",
    "cranfield_docs_url = \"cranfield/cran_docs.json\"\n",
    "cranfield_queries_url = \"cranfield/cran_queries.json\"\n",
    "\n",
    "docs,types_docs = url_to_corpus(cranfield_docs_url,'body')\n",
    "queries,types_queries = url_to_corpus(cranfield_queries_url,'query')\n",
    "\n",
    "types = list(types_docs.union(types_queries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "model_fasttext = gensim.models.KeyedVectors.load_word2vec_format(\"fasttext.vec\", binary=False)\n",
    "# model_glove = gensim.models.KeyedVectors.load_word2vec_format('glove.6B.300d.txt', binary=False)\n",
    "model_word2vec = gensim.models.KeyedVectors.load_word2vec_format('pretrained_vectors\\GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities import docs_to_embeddings\n",
    "\n",
    "doc_embeddings_fasttext = docs_to_embeddings(docs, model_fasttext)\n",
    "doc_embeddings_word2vec = docs_to_embeddings(docs, model_word2vec)\n",
    "query_embeddings_fasttext = docs_to_embeddings(queries, model_fasttext)\n",
    "query_embeddings_word2vec = docs_to_embeddings(queries, model_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "from math import log\n",
    "\n",
    "def compute_pmi(cooc_matrix):\n",
    "    N = np.sum(cooc_matrix)\n",
    "    row_totals = np.sum(cooc_matrix, axis=1)\n",
    "    col_totals = np.sum(cooc_matrix, axis=0)\n",
    "\n",
    "    pmi_matrix = np.zeros(cooc_matrix.shape)\n",
    "\n",
    "    for i in range(cooc_matrix.shape[0]):\n",
    "        for j in range(cooc_matrix.shape[1]):\n",
    "            pmi = np.log((cooc_matrix[i,j]*N)/((row_totals[i] + 0.1)*(col_totals[j] + 0.1)))\n",
    "            pmi_matrix[i,j] = max(0, pmi)\n",
    "\n",
    "    return pmi_matrix\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "documents = [' '.join(doc) for doc in docs]\n",
    "vectorizer = CountVectorizer()\n",
    "cooc_matrix = vectorizer.fit_transform(documents).T.dot(vectorizer.fit_transform(documents)).toarray()\n",
    "pmi_matrix = compute_pmi(cooc_matrix)\n",
    "tfidf_matrix = tfidf.fit_transform(documents)\n",
    "\n",
    "svd = TruncatedSVD(n_components=300)\n",
    "embedding_matrix = svd.fit_transform(pmi_matrix)\n",
    "embeddings_svd = normalize(embedding_matrix, norm='l2', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "doc_embeddings = []\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf.fit_transform(documents)\n",
    "\n",
    "for i in range(len(documents)):\n",
    "\n",
    "    doc_embedding = np.zeros(300)\n",
    "    tfidf_sum = 0\n",
    "\n",
    "    for j, term in enumerate(vectorizer.get_feature_names()):\n",
    "        \n",
    "        tfidf_sum += tfidf_matrix[i, j]\n",
    "        doc_embedding += embedding_matrix[j] * tfidf_matrix[i, j]\n",
    "    \n",
    "    if tfidf_sum != 0: doc_embedding /= tfidf_sum\n",
    "    doc_embeddings.append(doc_embedding)\n",
    "\n",
    "doc_embeddings_svd = np.array(doc_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "doc_embeddings_fasttext = np.genfromtxt('cranfield_embeddings/doc_embeddings_fasttext.csv',dtype=float )\n",
    "query_embeddings_fasttext = np.genfromtxt('cranfield_embeddings/query_embeddings_fasttext.csv',dtype=float )\n",
    "doc_embeddings_svd = np.genfromtxt('cranfield_embeddings/doc_embeddings_svd.csv',dtype=float )\n",
    "query_embeddings_svd = np.genfromtxt('cranfield_embeddings/query_embeddings_svd.csv',dtype=float )\n",
    "doc_embeddings_word2vec = np.genfromtxt('cranfield_embeddings/doc_embeddings_word2vec.csv',dtype=float )\n",
    "query_embeddings_word2vec = np.genfromtxt('cranfield_embeddings/query_embeddings_word2vec.csv',dtype=float )\n",
    "\n",
    "ss = StandardScaler()\n",
    "doc_embeddings_fasttext = ss.fit_transform(doc_embeddings_fasttext)\n",
    "ss = StandardScaler()\n",
    "query_embeddings_fasttext = ss.fit_transform(query_embeddings_fasttext)\n",
    "ss = StandardScaler()\n",
    "doc_embeddings_svd = ss.fit_transform(doc_embeddings_svd)\n",
    "ss = StandardScaler()\n",
    "query_embeddings_svd = ss.fit_transform(query_embeddings_svd)\n",
    "ss = StandardScaler()\n",
    "doc_embeddings_word2vec = ss.fit_transform(doc_embeddings_word2vec)\n",
    "ss = StandardScaler()\n",
    "query_embeddings_word2vec = ss.fit_transform(query_embeddings_word2vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elbow @ 20 for KMeans\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# kmeans_fasttext = []\n",
    "# gmm_fasttext = []\n",
    "# kmeans_word2vec = []\n",
    "# gmm_word2vec = []\n",
    "kmeans_svd = []\n",
    "gmm_svd = []\n",
    "\n",
    "for k in range(2, 50):\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=k, random_state=42).fit(doc_embeddings_fasttext)\n",
    "    kmeans_fasttext.append(kmeans.inertia_)\n",
    "\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42).fit(doc_embeddings_svd)\n",
    "    kmeans_svd.append(kmeans.inertia_)\n",
    "\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42).fit(doc_embeddings_word2vec)\n",
    "    kmeans_word2vec.append(kmeans.inertia_)\n",
    "\n",
    "    gmm = GaussianMixture(n_components=k, random_state=42).fit(doc_embeddings_fasttext)\n",
    "    cluster_assignments = gmm.predict(doc_embeddings_fasttext)\n",
    "    centroids = np.array([doc_embeddings_fasttext[cluster_assignments == i].mean(axis=0) for i in range(k)])\n",
    "    distortion = np.sum(np.square(doc_embeddings_fasttext - centroids[cluster_assignments]))\n",
    "    gmm_fasttext.append(distortion)\n",
    "\n",
    "    gmm = GaussianMixture(n_components=k, random_state=42).fit(doc_embeddings_svd)\n",
    "    cluster_assignments = gmm.predict(doc_embeddings_svd)\n",
    "    centroids = np.array([doc_embeddings_svd[cluster_assignments == i].mean(axis=0) for i in range(k)])\n",
    "    distortion = np.sum(np.square(doc_embeddings_svd - centroids[cluster_assignments]))\n",
    "    gmm_svd.append(distortion)\n",
    "\n",
    "    gmm = GaussianMixture(n_components=k, random_state=42).fit(doc_embeddings_word2vec)\n",
    "    cluster_assignments = gmm.predict(doc_embeddings_word2vec)\n",
    "    centroids = np.array([doc_embeddings_word2vec[cluster_assignments == i].mean(axis=0) for i in range(k)])\n",
    "    distortion = np.sum(np.square(doc_embeddings_word2vec - centroids[cluster_assignments]))\n",
    "    gmm_word2vec.append(distortion)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "plt.plot(range(2, 50), kmeans_fasttext, label = \"K_Means_fasttext\")\n",
    "plt.plot(range(2, 50), gmm_fasttext, label = \"GMM_fasttext\")\n",
    "\n",
    "plt.plot(range(2, 50), kmeans_svd, label = \"K_Means_svd\")\n",
    "plt.plot(range(2, 50), gmm_svd, label = \"GMM_svd\")\n",
    "\n",
    "plt.plot(range(2, 50), kmeans_word2vec, label = \"K_Means_word2vec\")\n",
    "plt.plot(range(2, 50), gmm_word2vec, label = \"GMM_word2vec\")\n",
    "\n",
    "plt.title('Objective function vs Number of Clusters')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Objective function')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=10, random_state=42).fit(doc_embeddings_fasttext)\n",
    "\n",
    "doc_clusters = kmeans.predict(doc_embeddings_fasttext)\n",
    "centroids = kmeans.cluster_centers_\n",
    "doc_indices = {tuple(centroid): np.where(doc_clusters == i)[0].tolist() for i, centroid in enumerate(centroids)}\n",
    "\n",
    "centroid_mapping_fasttext = doc_indices\n",
    "\n",
    "kmeans = KMeans(n_clusters=10, random_state=42).fit(doc_embeddings_word2vec)\n",
    "\n",
    "doc_clusters = kmeans.predict(doc_embeddings_word2vec)\n",
    "centroids = kmeans.cluster_centers_\n",
    "doc_indices = {tuple(centroid): np.where(doc_clusters == i)[0].tolist() for i, centroid in enumerate(centroids)}\n",
    "\n",
    "centroid_mapping_word2vec = doc_indices\n",
    "\n",
    "kmeans = KMeans(n_clusters=10, random_state=42).fit(doc_embeddings_svd)\n",
    "\n",
    "doc_clusters = kmeans.predict(doc_embeddings_svd)\n",
    "centroids = kmeans.cluster_centers_\n",
    "doc_indices = {tuple(centroid): np.where(doc_clusters == i)[0].tolist() for i, centroid in enumerate(centroids)}\n",
    "\n",
    "centroid_mapping_svd = doc_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cos_sim(a,b):\n",
    "\n",
    "    return dot(a, b)/(norm(a)*norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_result(query_embeddings, doc_embeddings, centroid_mapping):\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for i, query_embedding in enumerate(query_embeddings):\n",
    "\n",
    "        distances = [cos_sim(query_embedding,centroid) for centroid in centroid_mapping]\n",
    "        nearest_centroid = np.argmin(distances)\n",
    "        nearest_docs = centroid_mapping[list(centroid_mapping.keys())[nearest_centroid]]\n",
    "        similarities = [cos_sim(doc_embedding,query_embedding) for doc_embedding in doc_embeddings[nearest_docs]]\n",
    "        sorted_indices = np.argsort(similarities)[::-1]\n",
    "        sorted_docs = [nearest_docs[i] for i in sorted_indices]\n",
    "        results.append(sorted_docs)\n",
    "\n",
    "    return results\n",
    "\n",
    "svd_results = gen_result(query_embeddings_svd, doc_embeddings_svd, centroid_mapping_svd)\n",
    "w2v_results = gen_result(query_embeddings_word2vec, doc_embeddings_word2vec, centroid_mapping_word2vec)\n",
    "ft_results = gen_result(query_embeddings_fasttext, doc_embeddings_fasttext, centroid_mapping_fasttext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def types_to_idx(types):\n",
    "\n",
    "    seq_idx = {}\n",
    "\n",
    "    for t in types : seq_idx[t] = len(seq_idx)\n",
    "\n",
    "    return seq_idx\n",
    "\n",
    "seq_idx = types_to_idx(types)\n",
    "seq_idx['/start'] = len(seq_idx)\n",
    "seq_idx['/end'] = len(seq_idx)\n",
    "seq_idx['/unknown'] = len(seq_idx)\n",
    "seq_idx['/pad'] = len(seq_idx)\n",
    "\n",
    "def doc_to_seq(docs):\n",
    "    \n",
    "    for i in range(len(docs)):\n",
    "       \n",
    "        docs[i] = ['/start'] + docs[i] + ['/end']\n",
    "        docs[i] = [seq_idx.get(word, seq_idx['/unknown']) for word in docs[i]]\n",
    "\n",
    "    return docs\n",
    "\n",
    "doc_seq = doc_to_seq(docs)\n",
    "query_seq = doc_to_seq(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(seq_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = 9192\n",
    "NUM_LAYERS = 3\n",
    "HIDDEN_SIZE = 128\n",
    "EMBEDDING_SIZE = 128\n",
    "CELL_TYPE = \"LSTM\"\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class RNNEmbedding(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, embedding_size, cell_type):\n",
    "        super(RNNEmbedding, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.cell_type = cell_type\n",
    "        \n",
    "        # Define RNN cell type based on user input\n",
    "        if cell_type == \"RNN\":\n",
    "            self.rnn = nn.RNN(embedding_size, hidden_size, num_layers, batch_first=True)\n",
    "        elif cell_type == \"LSTM\":\n",
    "            self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, batch_first=True)\n",
    "        elif cell_type == \"GRU\":\n",
    "            self.rnn = nn.GRU(embedding_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "\n",
    "    def forward(self, doc_sequences):\n",
    "        \n",
    "        doc_sequences = torch.tensor(doc_sequences)\n",
    "        embedded = self.embedding(doc_sequences)\n",
    "\n",
    "        \n",
    "        if self.cell_type == \"LSTM\":\n",
    "            output, (hidden, cell) = self.rnn(embedded)\n",
    "        else:\n",
    "            output, hidden = self.rnn(embedded)\n",
    "\n",
    "        doc_embedding = hidden[-1]\n",
    "\n",
    "        return doc_embedding\n",
    "\n",
    "    \n",
    "embedding = RNNEmbedding(INPUT_SIZE, HIDDEN_SIZE, NUM_LAYERS, EMBEDDING_SIZE, CELL_TYPE)\n",
    "doc_seq_embeddings = np.array([embedding(seq).detach().numpy() for seq in doc_seq])\n",
    "query_seq_embeddings = np.array([embedding(seq).detach().numpy() for seq in query_seq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "doc_embeddings_2d = tsne.fit_transform(doc_seq_embeddings)\n",
    "plt.scatter(doc_embeddings_2d[:,0], doc_embeddings_2d[:,1], label=\"DOCS\")\n",
    "\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "doc_embeddings_2d = tsne.fit_transform(query_seq_embeddings)\n",
    "plt.scatter(doc_embeddings_2d[:,0], doc_embeddings_2d[:,1], label=\"QUERIES\")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "doc_seq_embeddings_np_2d = pca.fit_transform(doc_seq_embeddings)\n",
    "plt.scatter(doc_seq_embeddings_np_2d[:, 0], doc_seq_embeddings_np_2d[:, 1], label=\"docs\")\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "doc_seq_embeddings_np_2d = pca.fit_transform(query_seq_embeddings)\n",
    "plt.scatter(doc_seq_embeddings_np_2d[:, 0], doc_seq_embeddings_np_2d[:, 1], label=\"queries\")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('cranfield\\cran_qrels.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "X = [[int(d['query_num']), int(d['id'])] for d in data]\n",
    "y = [int(d['position']) for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_to_id(url,mode):\n",
    "\n",
    "  with open(url, 'r') as f: data = json.load(f)\n",
    "\n",
    "  num_docs = len(data)\n",
    "  docs = np.empty(num_docs, dtype='object')\n",
    "\n",
    "  for i in range(num_docs): docs[i] = int(data[i][mode])\n",
    "\n",
    "  return docs\n",
    "\n",
    "doc_id = url_to_id(\"cranfield\\cran_docs.json\",\"id\")\n",
    "query_id = url_to_id(\"cranfield\\cran_queries.json\",\"query number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 0\n",
    "\n",
    "for seq in docs:\n",
    "    if len(seq) > max_seq_length : max_seq_length = len(seq)\n",
    "\n",
    "for seq in queries:\n",
    "    if len(seq) > max_seq_length : max_seq_length = len(seq)\n",
    "\n",
    "print(max_seq_length)\n",
    "\n",
    "def types_to_idx(types):\n",
    "\n",
    "    seq_idx = {}\n",
    "\n",
    "    for t in types : seq_idx[t] = len(seq_idx)\n",
    "\n",
    "    return seq_idx\n",
    "\n",
    "seq_idx = types_to_idx(types)\n",
    "seq_idx['/start'] = len(seq_idx)\n",
    "seq_idx['/end'] = len(seq_idx)\n",
    "seq_idx['/unknown'] = len(seq_idx)\n",
    "seq_idx['/pad'] = len(seq_idx)\n",
    "\n",
    "print(seq_idx)\n",
    "\n",
    "def doc_to_seq(docs, seq_idx, max_seq_length, mode):\n",
    "\n",
    "    seqs = []\n",
    "    \n",
    "    for doc in docs:\n",
    "\n",
    "        seq = []\n",
    "        seq += doc\n",
    "        \n",
    "        if(mode=='pad') : \n",
    "            \n",
    "            print(\"Done\")\n",
    "            seq.insert(0,'/start')\n",
    "            seq.append('/end')\n",
    "            while(len(seq)<max_seq_length) : seq.insert(-1,'/pad')\n",
    "\n",
    "        seq = [seq_idx[word] for word in seq]\n",
    "        seqs.append(seq)\n",
    "        \n",
    "    return seqs\n",
    "\n",
    "doc_seq = np.array(doc_to_seq(docs, seq_idx, max_seq_length, 'pad'))\n",
    "#query_seq = np.array(doc_to_seq(queries, seq_idx, max_seq_length, 'pad'))\n",
    "\n",
    "print(doc_seq.shape)\n",
    "print(query_seq.shape)\n",
    "\n",
    "np.savetxt('cranfield_sequences/doc_seq.csv', doc_seq, fmt='%s')\n",
    "np.savetxt('cranfield_sequences/q_seq.csv', query_seq, fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_seq[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "tokens = docs + queries\n",
    "tokens = [f\"<start> {seq} <end>\" for seq in tokens]  # adding '/start' and '/end' to every seq\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(tokens)\n",
    "sequences = tokenizer.texts_to_sequences(tokens)\n",
    "max_length = 380\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "np.savetxt('cranfield_sequences/doc_seq.csv', padded_sequences[:len(docs)], fmt='%s')\n",
    "np.savetxt('cranfield_sequences/q_seq.csv', padded_sequences[len(docs):], fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embeddings = np.zeros((9000,300))\n",
    "\n",
    "token = 1\n",
    "\n",
    "for i in range(len(tokenizer.index_word)):\n",
    "\n",
    "    if(str(tokenizer.index_word[i+1]).strip(\"'\") in model_word2vec) : pretrained_embeddings[i+1] = model_word2vec[str(tokenizer.index_word[i+1]).strip(\"'\")]\n",
    "\n",
    "print(pretrained_embeddings.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('pretrained_embeddings.csv', pretrained_embeddings, fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities import train\n",
    "\n",
    "train(doc_embeddings_fasttext, query_embeddings_fasttext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(doc_embeddings_word2vec, query_embeddings_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(doc_embeddings_svd, query_embeddings_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities import clustering\n",
    "\n",
    "def clustering(X, random_state, r):\n",
    "\n",
    "    n_clusters_range = r\n",
    "    silhouette_scores = []\n",
    "    db_scores = []\n",
    "    ch_scores = []\n",
    "    min_docs = []\n",
    "    max_docs = []\n",
    "    K = []\n",
    "\n",
    "    for n_clusters in tqdm(n_clusters_range):\n",
    "        \n",
    "        gmm = GaussianMixture(n_components=n_clusters, random_state=random_state, max_iter = 500)\n",
    "        gmm.fit(X)\n",
    "        silhouette_scores.append(silhouette_score(X, gmm.predict(X)))\n",
    "        db_scores.append(davies_bouldin_score(X, gmm.predict(X)))\n",
    "        ch_scores.append(calinski_harabasz_score(X, gmm.predict(X)))\n",
    "\n",
    "        # Plotting minimum documents per cluster as well\n",
    "\n",
    "        doc_clusters = gmm.predict(X)\n",
    "        centroids = gmm.means_\n",
    "        doc_indices = {tuple(centroid): np.where(doc_clusters == i)[0].tolist() for i, centroid in enumerate(centroids)}\n",
    "\n",
    "        centroid_mapping_svd = doc_indices\n",
    "    \n",
    "        min_docs_per_cluster = 9999\n",
    "        max_docs_per_cluster = 0\n",
    "\n",
    "        for i in range(n_clusters):\n",
    "\n",
    "            min_docs_per_cluster = min(min_docs_per_cluster,(len(centroid_mapping_svd[list(centroid_mapping_svd.keys())[i]])))\n",
    "            max_docs_per_cluster = max(max_docs_per_cluster,(len(centroid_mapping_svd[list(centroid_mapping_svd.keys())[i]])))\n",
    "\n",
    "        min_docs.append(min_docs_per_cluster)\n",
    "        max_docs.append(max_docs_per_cluster)\n",
    "        K.append(n_clusters)\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=3, ncols=2, figsize=(15, 15))\n",
    "\n",
    "    silhouette_scores_norm = (silhouette_scores - np.min(silhouette_scores)) / (np.max(silhouette_scores) - np.min(silhouette_scores))\n",
    "    db_scores_norm = (db_scores - np.min(db_scores)) / (np.max(db_scores) - np.min(db_scores))\n",
    "    ch_scores_norm = (ch_scores - np.min(ch_scores)) / (np.max(ch_scores) - np.min(ch_scores))\n",
    "    avg_scores = (silhouette_scores_norm + ch_scores_norm + (1 - db_scores_norm)) / 3\n",
    "\n",
    "    ax[0, 0].plot(n_clusters_range, silhouette_scores, 'bo-')\n",
    "    ax[0, 0].set_xlabel('Number of clusters')\n",
    "    ax[0, 0].set_ylabel('Silhouette score')\n",
    "\n",
    "    ax[0, 1].plot(n_clusters_range, db_scores, 'bo-')\n",
    "    ax[0, 1].set_xlabel('Number of clusters')\n",
    "    ax[0, 1].set_ylabel('Davies-Bouldin index')\n",
    "\n",
    "    ax[1, 0].plot(n_clusters_range, ch_scores, 'bo-')\n",
    "    ax[1, 0].set_xlabel('Number of clusters')\n",
    "    ax[1, 0].set_ylabel('Calinski-Harabasz index')\n",
    "\n",
    "    ax[1, 1].plot(n_clusters_range, avg_scores, 'bo-')\n",
    "    ax[1, 1].set_xlabel('Number of clusters')\n",
    "    ax[1, 1].set_ylabel('Average Score')\n",
    "\n",
    "    ax[2, 0].plot(n_clusters_range, min_docs, 'bo-')\n",
    "    ax[2, 0].set_xlabel('Number of clusters')\n",
    "    ax[2, 0].set_ylabel('Minimum docs per cluster')\n",
    "\n",
    "    ax[2, 1].plot(n_clusters_range, max_docs, 'bo-')\n",
    "    ax[2, 1].set_xlabel('Number of clusters')\n",
    "    ax[2, 1].set_ylabel('Maximum docs per cluster')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering(doc_embeddings_fasttext, 42, range(2,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering(doc_embeddings_word2vec, 42, range(2,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering(doc_embeddings_svd, 42, range(20,41))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities import pickel_dictionary\n",
    "\n",
    "pickel_dictionary(doc_embeddings_svd , 'pickel_dictionaries\\centroid_mapping_svd.pkl', 30, 42)\n",
    "pickel_dictionary(doc_embeddings_word2vec , 'pickel_dictionaries\\centroid_mapping_word2vec.pkl', 15, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities import cluster_2d\n",
    "cluster_2d(doc_embeddings_svd, 30, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_2d(doc_embeddings_word2vec, 15, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "all_texts = [' '.join(doc) for doc in docs] + [' '.join(query) for query in queries]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_vectors = vectorizer.fit_transform(all_texts)\n",
    "\n",
    "doc_vectors = tfidf_vectors[:len(docs), :]\n",
    "query_vectors = tfidf_vectors[len(docs):, :]\n",
    "\n",
    "np.savetxt('cranfield_embeddings/doc_embeddings_vs.csv', doc_vectors.toarray(), fmt=\"%s\")\n",
    "np.savetxt('cranfield_embeddings/query_embeddings_vs.csv', query_vectors.toarray(), fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utilities import clustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "ss = StandardScaler()\n",
    "doc_embeddings_svd = ss.fit_transform(np.genfromtxt('cranfield_embeddings\\doc_embeddings_svd.csv', dtype = float))\n",
    "\n",
    "ss = StandardScaler()\n",
    "doc_embeddings_word2vec = ss.fit_transform(np.genfromtxt('cranfield_embeddings\\doc_embeddings_word2vec.csv', dtype = float))\n",
    "\n",
    "ss = StandardScaler()\n",
    "doc_embeddings_seq = ss.fit_transform(np.genfromtxt('cranfield_sequences\\doc_seq_embedding.csv', dtype = float))\n",
    "\n",
    "ss = StandardScaler()\n",
    "doc_embeddings_seq_2 = ss.fit_transform(np.genfromtxt('cranfield_sequences\\doc_seq_embedding_2.csv', dtype = float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering(doc_embeddings_svd, 42, (2,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
